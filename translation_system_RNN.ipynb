{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e0abb4-21bd-4e67-873d-87424c18577f",
   "metadata": {},
   "source": [
    "# Translation system to translate English to french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b82ef55d-c6ea-4177-933e-9827e3055478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10ef6fe-db24-4f67-a6e4-4c3232247b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'In any event , this ...' -> 'En tout cas , cette ...'>\n",
      "Resumption\n",
      "of\n",
      "the\n",
      "session\n",
      "I\n",
      "declare\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# nltk.download('comtrans')\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[54])\n",
    "words = comtrans.words(\"alignment-en-fr.txt\")\n",
    "\n",
    "for word in words[:6]:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c757f1-d79a-448d-b939-49e0fd16e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n",
      "['Reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].words)\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].mots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8895be-22a1-4d9c-b493-5dc524afc337",
   "metadata": {},
   "source": [
    "## Preprocessing the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82aeefca-e25b-4fad-b799-c6478ac2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import comtrans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b56c8-bf87-4d25-9926-4a960cf2dbf3",
   "metadata": {},
   "source": [
    "### Function to retrieve the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d75fe3a-9c69-48c1-84e8-632c86206308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes an argument for the languages\n",
    "\n",
    "\n",
    "def retrieve_corpora(translated_sentences_l1_l2=\"alignment-en-fr.txt\"):\n",
    "    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n",
    "    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n",
    "    sentences_l1 = [sent.words for sent in als]  # store the english sentences\n",
    "    sentences_l2 = [sent.mots for sent in als]  # store the french sentences\n",
    "    return sentences_l1, sentences_l2  # return the list of both languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6169f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving corpora: alignment-en-fr.txt\n",
      "# A sentence in the two languages English and French\n",
      "English:  [['Resumption', 'of', 'the', 'session'], ['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.'], ['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.'], ['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.'], ['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']]\n",
      "French:  [['Reprise', 'de', 'la', 'session'], ['Je', 'déclare', 'reprise', 'la', 'session', 'du', 'Parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', '17', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances', '.'], ['Vous', 'avez', 'souhaité', 'un', 'débat', 'à', 'ce', 'sujet', 'dans', 'les', 'prochains', 'jours', ',', 'au', 'cours', 'de', 'cette', 'période', 'de', 'session', '.'], ['Je', 'vous', 'invite', 'à', 'vous', 'lever', 'pour', 'cette', 'minute', 'de', 'silence', '.'], ['(', 'Le', 'Parlement', ',', 'debout', ',', 'observe', 'une', 'minute', 'de', 'silence', ')']]\n",
      "# Corpora Length (Number of Sentences)\n",
      "33334\n",
      "33334\n"
     ]
    }
   ],
   "source": [
    "## Testing function\n",
    "sen_l1, sen_l2 = retrieve_corpora()\n",
    "print(\"# A sentence in the two languages English and French\")\n",
    "print(\"English: \", sen_l1[:5])  # first 5 Sentences in English\n",
    "print(\"French: \", sen_l2[:5])  # first 5 Sentences in French\n",
    "\n",
    "print(\"# Corpora Length (Number of Sentences)\")\n",
    "print(len(sen_l1))\n",
    "print(len(sen_l2))\n",
    "\n",
    "assert len(sen_l1) == len(sen_l2)  # check if the number of sentences are the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08034f8",
   "metadata": {},
   "source": [
    "# clean up tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f662ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  ['resumption', 'of', 'the', 'session']\n",
      "French:  ['reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "def clean_sentences(sentence):\n",
    "    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n",
    "    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n",
    "    return [w for words in clean_words for w in words if words if w]\n",
    "\n",
    "\n",
    "# testing function\n",
    "clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "clean_sen_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "\n",
    "print(\"English: \", clean_sen_l1[0])\n",
    "print(\"French: \", clean_sen_l2[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f206",
   "metadata": {},
   "source": [
    "### Filtering the sentences that are too long to be processed due to limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d38cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Filterd Corpora length\n",
      "13405\n"
     ]
    }
   ],
   "source": [
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if (\n",
    "            min_len <= len(sentences_l1[i]) <= max_len\n",
    "            and min_len <= len(sentences_l2[i]) <= max_len\n",
    "        ):\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "\n",
    "    return filtered_sentences_l1, filtered_sentences_l2\n",
    "\n",
    "\n",
    "# Testing\n",
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "    clean_sen_l1, clean_sen_l2\n",
    ")\n",
    "\n",
    "# checking how many sentences made it through the filter (13,405 survived)\n",
    "print(\"# Filterd Corpora length\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7572f71",
   "metadata": {},
   "source": [
    "# Move text to numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0b738",
   "metadata": {},
   "source": [
    "### Create a dictionary of words for each language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aef5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"  # For padding\n",
    "_GO = \"_GO\"  # to divide 2 sentences\n",
    "_EOS = \"_EOS\"  # to indicate where sentence stops\n",
    "_UNK = \"_UNK\"  # for unknown words\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "OP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09090ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_utils\n",
    "\n",
    "\n",
    "# def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "#     count_words = Counter()\n",
    "#     dict_words = {}\n",
    "#     opt_dict_size = len(data_utils.OP_DICT_IDS)\n",
    "#     for sen in sentences:\n",
    "#         for word in sen:\n",
    "#             count_words[word] += 1\n",
    "\n",
    "#     dict_words[data_utils._PAD] = data_utils.PAD_ID\n",
    "#     dict_words[data_utils._GO] = data_utils._GO_ID\n",
    "#     dict_words[data_utils._EOS] = data_utils._EOS_ID\n",
    "#     dict_words[data_utils._UNK] = data_utils._UNK_ID\n",
    "\n",
    "#     for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "#         dict_words[item[0]] == idx + opt_dict_size\n",
    "#     if storage_path:\n",
    "#         pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
    "#     return dict_words\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()  # to count the number of times the words occur\n",
    "    dict_words = {}\n",
    "\n",
    "    # Initial dictionary size, based on the number of special tokens\n",
    "    opt_dict_size = 4  # Since we have 4 special tokens (PAD, GO, EOS, UNK)\n",
    "\n",
    "    # Count the frequency of each word in the sentences\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1  # add one using the word as the key\n",
    "\n",
    "    # Add special tokens to the dictionary with predefined IDs\n",
    "    dict_words[_PAD] = PAD_ID\n",
    "    dict_words[_GO] = GO_ID\n",
    "    dict_words[_EOS] = EOS_ID\n",
    "    dict_words[_UNK] = UNK_ID\n",
    "\n",
    "    # Add the most common words from the count to the dictionary\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + opt_dict_size\n",
    "\n",
    "    # print(\"shambala\",dict_words)\n",
    "    # print(\"shamb\", count_words)\n",
    "    # Save the dictionary to a file if a storage path is provided\n",
    "    if storage_path:\n",
    "        with open(storage_path, \"wb\") as f:\n",
    "            pickle.dump(dict_words, f)\n",
    "\n",
    "    return dict_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa081c",
   "metadata": {},
   "source": [
    "### Look up tokens and substitute them with their token ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d1bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "#     indexed_sentences = []\n",
    "#     not_found_counter = 0\n",
    "#     for sent in sentences:\n",
    "#         idx_sent = []\n",
    "#     for word in sent:\n",
    "#         try:\n",
    "#             idx_sent.append(indexed_dictionary[word])\n",
    "#         except KeyError:\n",
    "#             idx_sent.append(data_utils.UNK_ID)\n",
    "#             not_found_counter += 1\n",
    "#     indexed_sentences.append(idx_sent)\n",
    "#     print(\"[sentences_to_indexes] Did not find {} words\".format(not_found_counter))\n",
    "#     return indexed_sentences\n",
    "\n",
    "\n",
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "\n",
    "    # Define UNK_ID for unknown words\n",
    "    UNK_ID = 3  # Assuming UNK_ID is 3 as defined earlier\n",
    "\n",
    "    # Convert each sentence into its indexed form\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "        for word in sent:\n",
    "            # Use the dictionary to find the index of the word, or UNK_ID if not found\n",
    "            idx_sent.append(indexed_dictionary.get(word, UNK_ID))\n",
    "        indexed_sentences.append(idx_sent)\n",
    "\n",
    "    print(\"[sentences_to_indexes] Did not find {} words\".format(not_found_counter))\n",
    "    return indexed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2542e8b",
   "metadata": {},
   "source": [
    "### testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae83601",
   "metadata": {},
   "source": [
    "### subsititute tokens with their ID and if Token is not in the dictionary, the ID of unknown is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9de90ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sentences_to_indexes] Did not find 0 words\n",
      "[sentences_to_indexes] Did not find 0 words\n",
      "# Same sentences as before, with their dictionary ID\n",
      "English: [('resumption', 1382), ('of', 9), ('the', 5), ('session', 459)]\n",
      "French: [('reprise', 840), ('de', 6), ('la', 8), ('session', 466)]\n"
     ]
    }
   ],
   "source": [
    "dict_l1 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\"\n",
    ")\n",
    "dict_l2 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l2, dict_size=10000, storage_path=\"/tmp/l2_dict.p\"\n",
    ")\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"English:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
    "print(\"French:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c6d17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Max sentence lengthsL:\n",
      "English 20\n",
      "French 20\n"
     ]
    }
   ],
   "source": [
    "# function to get the maximum size\n",
    "def extract_max_length(corpora):\n",
    "    return max([len(sentence) for sentence in corpora])\n",
    "\n",
    "\n",
    "max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "print(\"# Max sentence lengthsL:\")\n",
    "print(\"English\", max_length_l1)\n",
    "print(\"French\", max_length_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4f2a7",
   "metadata": {},
   "source": [
    "## pad the sequences to be the same length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c373db",
   "metadata": {},
   "source": [
    "### padd input to be 20 symbols long and output to be 20 symbols long\n",
    "\n",
    "### insert \\_GO t the beginning of the output sentence and \\_EOS at the end to position the start and the end of the translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "342485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "#     assert len(sentences_l1) == len(sentences_l2)\n",
    "#     data_set = []\n",
    "#     for i in range(len(sentences_l1)):\n",
    "#         padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "#         pad_sentence_l1 = ([data_utils.PAD_ID] * padding_l1) + sentences_l1[i]\n",
    "#         padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "#         pad_sentence_l2 = (\n",
    "#             [data_utils.GO_ID]\n",
    "#             + sentences_l2[i]\n",
    "#             + [data_utils.EOS_ID]\n",
    "#             + ([data_utils.PAD_ID] * padding_l2)\n",
    "#         )\n",
    "\n",
    "#         data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "#     return data_set\n",
    "\n",
    "\n",
    "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "    assert len(sentences_l1) == len(sentences_l2)\n",
    "    data_set = []\n",
    "\n",
    "    for i in range(len(sentences_l1)):\n",
    "        # Pad sentences in the first language (l1) with PAD_ID\n",
    "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "        pad_sentence_l1 = ([PAD_ID] * padding_l1) + sentences_l1[i]\n",
    "\n",
    "        # Pad sentences in the second language (l2) with GO_ID at the start, EOS_ID at the end, and PAD_ID as needed\n",
    "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "        pad_sentence_l2 = [GO_ID] + sentences_l2[i] + [EOS_ID] + ([PAD_ID] * padding_l2)\n",
    "\n",
    "        # Add the padded sentence pair to the dataset\n",
    "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "739e908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prepared minibatch with paddings and extra stuff\n",
      "En: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1382, 9, 5, 459]\n",
      "Fr: [1, 840, 6, 8, 466, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "# The sentence pass from X to Y tokens\n",
      "English: 4 -> 20\n",
      "French: 4 -> 22\n"
     ]
    }
   ],
   "source": [
    "data_set = prepare_sentences(\n",
    "    idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    ")\n",
    "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
    "print(\"En:\", data_set[0][0])\n",
    "print(\"Fr:\", data_set[0][1])\n",
    "print(\"# The sentence pass from X to Y tokens\")\n",
    "print(\"English:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
    "print(\"French:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585590d6",
   "metadata": {},
   "source": [
    "## training the Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b66536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# from seq2seq_model import Seq2SeqModel\n",
    "# from corpora_tools import *\n",
    "\n",
    "path_l1_dict = \"/tmp/l1_dict.p\"\n",
    "path_l2_dict = \"/tmp/l2_dict.p\"\n",
    "model_dir = \"/tmp/translate \"\n",
    "model_checkpoints = model_dir + \"/translate.ckpt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6710f8",
   "metadata": {},
   "source": [
    "This function returns the cleaned sentences, the dataset, the maximum length\n",
    "of the sentences, and the lengths of the dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f3154ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(use_stored_dictionary=False):\n",
    "    sen_l1, senl2 = retrieve_corpora()\n",
    "    clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "    clean_sent_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "        clean_sen_l1, clean_sen_l2\n",
    "    )\n",
    "\n",
    "    if not use_stored_dictionary:\n",
    "        dict_l1 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=15000, storage_path=path_l1_dict\n",
    "        )\n",
    "        dict_l2 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=10000, storage_path=path_l2_dict\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "\n",
    "    dict_l1_length = len(dict_l1)\n",
    "    dict_l2_length = len(dict_l2)\n",
    "\n",
    "    data_set = prepare_sentences(\n",
    "        idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    "    )\n",
    "    return (\n",
    "        (filt_clean_sen_l1, filt_clean_sen_l2),\n",
    "        data_set,\n",
    "        (max_length_l1, max_length_l2),\n",
    "        (dict_l1_length, dict_l2_length),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32fad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_checkpoints(model_dirb, model_checkpoints):\n",
    "    for f in glob.glob(model_checkpoints + \"*\"):\n",
    "        os.remove(f)\n",
    "    try:\n",
    "        os.mkdir(model_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bb478ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13405"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
