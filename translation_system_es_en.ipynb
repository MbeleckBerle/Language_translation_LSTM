{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e0abb4-21bd-4e67-873d-87424c18577f",
   "metadata": {},
   "source": [
    "# Translation system to translate English to french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ef55d-c6ea-4177-933e-9827e3055478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10ef6fe-db24-4f67-a6e4-4c3232247b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'In any event , this ...' -> 'En tout cas , cette ...'>\n",
      "Resumption\n",
      "of\n",
      "the\n",
      "session\n",
      "I\n",
      "declare\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# nltk.download('comtrans')\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[54])\n",
    "words = comtrans.words(\"alignment-en-fr.txt\")\n",
    "\n",
    "for word in words[:6]:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c757f1-d79a-448d-b939-49e0fd16e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n",
      "['Reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].words)\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].mots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8895be-22a1-4d9c-b493-5dc524afc337",
   "metadata": {},
   "source": [
    "## Preprocessing the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82aeefca-e25b-4fad-b799-c6478ac2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import comtrans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b56c8-bf87-4d25-9926-4a960cf2dbf3",
   "metadata": {},
   "source": [
    "### Function to retrieve the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d75fe3a-9c69-48c1-84e8-632c86206308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes an argument for the languages\n",
    "def retrieve_corpora(translated_sentences_l1_l2=\"alignment-en-fr.txt\"):\n",
    "    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n",
    "    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n",
    "    sentences_l1 = [sent.words for sent in als]  # store the english sentences\n",
    "    sentences_l2 = [sent.mots for sent in als]  # store the french sentences\n",
    "    return sentences_l1, sentences_l2  # return the list of both languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6169f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving corpora: alignment-en-fr.txt\n",
      "# A sentence in the two languages English and French\n",
      "English:  [['Resumption', 'of', 'the', 'session'], ['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.'], ['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.'], ['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.'], ['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']]\n",
      "French:  [['Reprise', 'de', 'la', 'session'], ['Je', 'déclare', 'reprise', 'la', 'session', 'du', 'Parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', '17', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances', '.'], ['Vous', 'avez', 'souhaité', 'un', 'débat', 'à', 'ce', 'sujet', 'dans', 'les', 'prochains', 'jours', ',', 'au', 'cours', 'de', 'cette', 'période', 'de', 'session', '.'], ['Je', 'vous', 'invite', 'à', 'vous', 'lever', 'pour', 'cette', 'minute', 'de', 'silence', '.'], ['(', 'Le', 'Parlement', ',', 'debout', ',', 'observe', 'une', 'minute', 'de', 'silence', ')']]\n",
      "# Corpora Length (Number of Sentences)\n",
      "33334\n",
      "33334\n"
     ]
    }
   ],
   "source": [
    "## Testing function\n",
    "sen_l1, sen_l2 = retrieve_corpora()\n",
    "print(\"# A sentence in the two languages English and French\")\n",
    "print(\"English: \", sen_l1[:5])  # first 5 Sentences in English\n",
    "print(\"French: \", sen_l2[:5])  # first 5 Sentences in French\n",
    "\n",
    "print(\"# Corpora Length (Number of Sentences)\")\n",
    "print(len(sen_l1))\n",
    "print(len(sen_l2))\n",
    "\n",
    "assert len(sen_l1) == len(sen_l2)  # check if the number of sentences are the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08034f8",
   "metadata": {},
   "source": [
    "# clean up tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f662ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  ['resumption', 'of', 'the', 'session']\n",
      "French:  ['reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "def clean_sentences(sentence):\n",
    "    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n",
    "    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n",
    "    return [w for words in clean_words for w in words if words if w]\n",
    "\n",
    "\n",
    "# testing function\n",
    "clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "clean_sen_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "\n",
    "print(\"English: \", clean_sen_l1[0])\n",
    "print(\"French: \", clean_sen_l2[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f206",
   "metadata": {},
   "source": [
    "### Filtering the sentences that are too long to be processed due to limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d38cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Filterd Corpora length\n",
      "13405\n"
     ]
    }
   ],
   "source": [
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if (\n",
    "            min_len <= len(sentences_l1[i]) <= max_len\n",
    "            and min_len <= len(sentences_l2[i]) <= max_len\n",
    "        ):\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "\n",
    "    return filtered_sentences_l1, filtered_sentences_l2\n",
    "\n",
    "\n",
    "# Testing\n",
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "    clean_sen_l1, clean_sen_l2\n",
    ")\n",
    "\n",
    "# checking how many sentences made it through the filter (13,405 survived)\n",
    "print(\"# Filterd Corpora length\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7572f71",
   "metadata": {},
   "source": [
    "# Move text to numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0b738",
   "metadata": {},
   "source": [
    "### Create a dictionary of words for each language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aef5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"  # For padding\n",
    "_GO = \"_GO\"  # to divide 2 sentences\n",
    "_EOS = \"_EOS\"  # to indicate where sentence stops\n",
    "_UNK = \"_UNK\"  # for unknown words\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "OP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09090ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()\n",
    "    dict_words = {}\n",
    "    opt_dict_size = len(OP_DICT_IDS)\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1\n",
    "\n",
    "    dict_words[_PAD] = PAD_ID\n",
    "    dict_words[_GO] = GO_ID\n",
    "    dict_words[_EOS] = EOS_ID\n",
    "    dict_words[_UNK] = UNK_ID\n",
    "    # print(count_words.most_common(dict_size))\n",
    "\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + opt_dict_size\n",
    "    if storage_path:\n",
    "        pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
    "    return dict_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa081c",
   "metadata": {},
   "source": [
    "### Look up tokens and substitute them with their token ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d1bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            idx_sent.append(indexed_dictionary[word])\n",
    "        except KeyError:\n",
    "            idx_sent.append(UNK_ID)\n",
    "            not_found_counter += 1\n",
    "    indexed_sentences.append(idx_sent)\n",
    "    print(\"[sentences_to_indexes] Did not find {} words\".format(not_found_counter))\n",
    "    return indexed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2542e8b",
   "metadata": {},
   "source": [
    "### testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae83601",
   "metadata": {},
   "source": [
    "### subsititute tokens with their ID and if Token is not in the dictionary, the ID of unknown is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de90ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sentences_to_indexes] Did not find 0 words\n",
      "[sentences_to_indexes] Did not find 0 words\n",
      "# Same sentences as before, with their dictionary ID\n",
      "English: [('resumption', 168), ('of', 1308), ('the', 1239), ('session', 5)]\n",
      "French: [('reprise', 21), ('de', 472), ('la', 20), ('session', 5)]\n"
     ]
    }
   ],
   "source": [
    "dict_l1 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\"\n",
    ")\n",
    "dict_l2 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l2, dict_size=10000, storage_path=\"/tmp/l2_dict.p\"\n",
    ")\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"English:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
    "print(\"French:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6d17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Max sentence lengthsL:\n",
      "English 7\n",
      "French 9\n"
     ]
    }
   ],
   "source": [
    "# function to get the maximum size\n",
    "def extract_max_length(corpora):\n",
    "    return max([len(sentence) for sentence in corpora])\n",
    "\n",
    "\n",
    "max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "print(\"# Max sentence lengthsL:\")\n",
    "print(\"English\", max_length_l1)\n",
    "print(\"French\", max_length_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4f2a7",
   "metadata": {},
   "source": [
    "## pad the sequences to be the same length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c373db",
   "metadata": {},
   "source": [
    "### padd input to be 20 symbols long and output to be 20 symbols long\n",
    "\n",
    "### insert \\_GO t the beginning of the output sentence and \\_EOS at the end to position the start and the end of the translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "342485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "    assert len(sentences_l1) == len(sentences_l2)\n",
    "    data_set = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "        pad_sentence_l1 = ([PAD_ID] * padding_l1) + sentences_l1[i]\n",
    "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "        pad_sentence_l2 = [GO_ID] + sentences_l2[i] + [EOS_ID] + ([PAD_ID] * padding_l2)\n",
    "\n",
    "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739e908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prepared minibatch with paddings and extra stuff\n",
      "En: [168, 1308, 1239, 5, 556, 955, 4]\n",
      "Fr: [1, 21, 472, 20, 5, 1378, 11, 489, 1643, 4, 2]\n",
      "# The sentence pass from X to Y tokens\n",
      "English: 7 -> 7\n",
      "French: 9 -> 11\n"
     ]
    }
   ],
   "source": [
    "data_set = prepare_sentences(\n",
    "    idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    ")\n",
    "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
    "print(\"En:\", data_set[0][0])\n",
    "print(\"Fr:\", data_set[0][1])\n",
    "print(\"# The sentence pass from X to Y tokens\")\n",
    "print(\"English:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
    "print(\"French:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585590d6",
   "metadata": {},
   "source": [
    "## training the Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b66536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 04:44:19.759430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-16 04:44:19.768952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 04:44:19.778017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 04:44:19.780411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 04:44:19.787904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 04:44:20.629960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from seq2seq_model import Seq2SeqModel\n",
    "\n",
    "# from corpora_tools import *\n",
    "\n",
    "path_l1_dict = \"/tmp/l1_dict.p\"\n",
    "path_l2_dict = \"/tmp/l2_dict.p\"\n",
    "model_dir = \"/tmp/translate \"\n",
    "model_checkpoints = model_dir + \"/translate.ckpt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6710f8",
   "metadata": {},
   "source": [
    "This function returns the cleaned sentences, the dataset, the maximum length\n",
    "of the sentences, and the lengths of the dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f3154ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(use_stored_dictionary=False):\n",
    "    sen_l1, senl2 = retrieve_corpora()\n",
    "    clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "    clean_sent_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "        clean_sen_l1, clean_sen_l2\n",
    "    )\n",
    "\n",
    "    if not use_stored_dictionary:\n",
    "        dict_l1 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=15000, storage_path=path_l1_dict\n",
    "        )\n",
    "        dict_l2 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=10000, storage_path=path_l2_dict\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "\n",
    "    dict_l1_length = len(dict_l1)\n",
    "    dict_l2_length = len(dict_l2)\n",
    "\n",
    "    data_set = prepare_sentences(\n",
    "        idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    "    )\n",
    "    return (\n",
    "        (filt_clean_sen_l1, filt_clean_sen_l2),\n",
    "        data_set,\n",
    "        (max_length_l1, max_length_l2),\n",
    "        (dict_l1_length, dict_l2_length),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32fad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_checkpoints(model_dir, model_checkpoints):\n",
    "    for f in glob.glob(model_checkpoints + \"*\"):\n",
    "        os.remove(f)\n",
    "    try:\n",
    "        os.mkdir(model_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2f4e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b721e859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6d9c04e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:105\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:514\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a `TypeSpec` for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m     element,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [[[168, 1308, 1239, 5, 556, 955, 4], [1, 21, 472, 20, 5, 1378, 11, 489, 1643, 4, 2]]] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mlen\u001b[39m(data_set))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m---> 29\u001b[0m data_set \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2SeqModel(vocab_size_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dict_l1), vocab_size_tgt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dict_l2), embedding_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, max_length_src\u001b[38;5;241m=\u001b[39mmax_length_l1, max_length_tgt\u001b[38;5;241m=\u001b[39mmax_length_l2)\n\u001b[1;32m     31\u001b[0m train(model, data_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/translate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(data_set, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(data_set, batch_size):\n\u001b[0;32m---> 25\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mlen\u001b[39m(data_set))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:110\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 110\u001b[0m       \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;66;03m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;66;03m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
