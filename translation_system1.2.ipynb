{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e0abb4-21bd-4e67-873d-87424c18577f",
   "metadata": {},
   "source": [
    "# Translation system to translate English to french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ef55d-c6ea-4177-933e-9827e3055478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/mbeleck/anaconda3/envs/tf2-gpu/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10ef6fe-db24-4f67-a6e4-4c3232247b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'In any event , this ...' -> 'En tout cas , cette ...'>\n",
      "Resumption\n",
      "of\n",
      "the\n",
      "session\n",
      "I\n",
      "declare\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# nltk.download('comtrans')\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[54])\n",
    "words = comtrans.words(\"alignment-en-fr.txt\")\n",
    "\n",
    "for word in words[:6]:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c757f1-d79a-448d-b939-49e0fd16e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n",
      "['Reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].words)\n",
    "print(comtrans.aligned_sents(\"alignment-en-fr.txt\")[0].mots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8895be-22a1-4d9c-b493-5dc524afc337",
   "metadata": {},
   "source": [
    "## Preprocessing the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82aeefca-e25b-4fad-b799-c6478ac2646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import comtrans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b56c8-bf87-4d25-9926-4a960cf2dbf3",
   "metadata": {},
   "source": [
    "### Function to retrieve the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d75fe3a-9c69-48c1-84e8-632c86206308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes an argument for the languages\n",
    "def retrieve_corpora(translated_sentences_l1_l2=\"alignment-en-fr.txt\"):\n",
    "    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n",
    "    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n",
    "    sentences_l1 = [sent.words for sent in als]  # store the english sentences\n",
    "    sentences_l2 = [sent.mots for sent in als]  # store the french sentences\n",
    "    return sentences_l1, sentences_l2  # return the list of both languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6169f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving corpora: alignment-en-fr.txt\n",
      "# A sentence in the two languages English and French\n",
      "English:  [['Resumption', 'of', 'the', 'session'], ['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.'], ['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.'], ['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.'], ['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']]\n",
      "French:  [['Reprise', 'de', 'la', 'session'], ['Je', 'déclare', 'reprise', 'la', 'session', 'du', 'Parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', '17', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances', '.'], ['Vous', 'avez', 'souhaité', 'un', 'débat', 'à', 'ce', 'sujet', 'dans', 'les', 'prochains', 'jours', ',', 'au', 'cours', 'de', 'cette', 'période', 'de', 'session', '.'], ['Je', 'vous', 'invite', 'à', 'vous', 'lever', 'pour', 'cette', 'minute', 'de', 'silence', '.'], ['(', 'Le', 'Parlement', ',', 'debout', ',', 'observe', 'une', 'minute', 'de', 'silence', ')']]\n",
      "# Corpora Length (Number of Sentences)\n",
      "33334\n",
      "33334\n"
     ]
    }
   ],
   "source": [
    "## Testing function\n",
    "sen_l1, sen_l2 = retrieve_corpora()\n",
    "print(\"# A sentence in the two languages English and French\")\n",
    "print(\"English: \", sen_l1[:5])  # first 5 Sentences in English\n",
    "print(\"French: \", sen_l2[:5])  # first 5 Sentences in French\n",
    "\n",
    "print(\"# Corpora Length (Number of Sentences)\")\n",
    "print(len(sen_l1))\n",
    "print(len(sen_l2))\n",
    "\n",
    "assert len(sen_l1) == len(sen_l2)  # check if the number of sentences are the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08034f8",
   "metadata": {},
   "source": [
    "# clean up tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f662ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  ['resumption', 'of', 'the', 'session']\n",
      "French:  ['reprise', 'de', 'la', 'session']\n"
     ]
    }
   ],
   "source": [
    "def clean_sentences(sentence):\n",
    "    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n",
    "    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n",
    "    return [w for words in clean_words for w in words if words if w]\n",
    "\n",
    "\n",
    "# testing function\n",
    "clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "clean_sen_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "\n",
    "print(\"English: \", clean_sen_l1[0])\n",
    "print(\"French: \", clean_sen_l2[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f206",
   "metadata": {},
   "source": [
    "### Filtering the sentences that are too long to be processed due to limited resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d38cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Filterd Corpora length\n",
      "13405\n"
     ]
    }
   ],
   "source": [
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if (\n",
    "            min_len <= len(sentences_l1[i]) <= max_len\n",
    "            and min_len <= len(sentences_l2[i]) <= max_len\n",
    "        ):\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "\n",
    "    return filtered_sentences_l1, filtered_sentences_l2\n",
    "\n",
    "\n",
    "# Testing\n",
    "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "    clean_sen_l1, clean_sen_l2\n",
    ")\n",
    "\n",
    "# checking how many sentences made it through the filter (13,405 survived)\n",
    "print(\"# Filterd Corpora length\")\n",
    "print(len(filt_clean_sen_l1))\n",
    "\n",
    "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7572f71",
   "metadata": {},
   "source": [
    "# Move text to numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0b738",
   "metadata": {},
   "source": [
    "### Create a dictionary of words for each language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aef5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"  # For padding\n",
    "_GO = \"_GO\"  # to divide 2 sentences\n",
    "_EOS = \"_EOS\"  # to indicate where sentence stops\n",
    "_UNK = \"_UNK\"  # for unknown words\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "OP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09090ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
    "    count_words = Counter()\n",
    "    dict_words = {}\n",
    "    opt_dict_size = len(OP_DICT_IDS)\n",
    "    for sen in sentences:\n",
    "        for word in sen:\n",
    "            count_words[word] += 1\n",
    "\n",
    "    dict_words[_PAD] = PAD_ID\n",
    "    dict_words[_GO] = GO_ID\n",
    "    dict_words[_EOS] = EOS_ID\n",
    "    dict_words[_UNK] = UNK_ID\n",
    "    # print(count_words.most_common(dict_size))\n",
    "\n",
    "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
    "        dict_words[item[0]] = idx + opt_dict_size\n",
    "    if storage_path:\n",
    "        pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
    "    return dict_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa081c",
   "metadata": {},
   "source": [
    "### Look up tokens and substitute them with their token ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d1bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indexes(sentences, indexed_dictionary):\n",
    "    indexed_sentences = []\n",
    "    not_found_counter = 0\n",
    "    for sent in sentences:\n",
    "        idx_sent = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            idx_sent.append(indexed_dictionary[word])\n",
    "        except KeyError:\n",
    "            idx_sent.append(UNK_ID)\n",
    "            not_found_counter += 1\n",
    "    indexed_sentences.append(idx_sent)\n",
    "    print(\"[sentences_to_indexes] Did not find {} words\".format(not_found_counter))\n",
    "    return indexed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2542e8b",
   "metadata": {},
   "source": [
    "### testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae83601",
   "metadata": {},
   "source": [
    "### subsititute tokens with their ID and if Token is not in the dictionary, the ID of unknown is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de90ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sentences_to_indexes] Did not find 0 words\n",
      "[sentences_to_indexes] Did not find 0 words\n",
      "# Same sentences as before, with their dictionary ID\n",
      "English: [('resumption', 168), ('of', 1308), ('the', 1239), ('session', 5)]\n",
      "French: [('reprise', 21), ('de', 472), ('la', 20), ('session', 5)]\n"
     ]
    }
   ],
   "source": [
    "dict_l1 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\"\n",
    ")\n",
    "dict_l2 = create_indexed_dictionary(\n",
    "    filt_clean_sen_l2, dict_size=10000, storage_path=\"/tmp/l2_dict.p\"\n",
    ")\n",
    "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
    "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
    "print(\"# Same sentences as before, with their dictionary ID\")\n",
    "print(\"English:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
    "print(\"French:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6d17cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Max sentence lengthsL:\n",
      "English 7\n",
      "French 9\n"
     ]
    }
   ],
   "source": [
    "# function to get the maximum size\n",
    "def extract_max_length(corpora):\n",
    "    return max([len(sentence) for sentence in corpora])\n",
    "\n",
    "\n",
    "max_length_l1 = extract_max_length(idx_sentences_l1)\n",
    "max_length_l2 = extract_max_length(idx_sentences_l2)\n",
    "print(\"# Max sentence lengths:\")\n",
    "print(\"English\", max_length_l1)\n",
    "print(\"French\", max_length_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4f2a7",
   "metadata": {},
   "source": [
    "## pad the sequences to be the same length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c373db",
   "metadata": {},
   "source": [
    "### padd input to be 20 symbols long and output to be 20 symbols long\n",
    "\n",
    "### insert \\_GO t the beginning of the output sentence and \\_EOS at the end to position the start and the end of the translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "342485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
    "    assert len(sentences_l1) == len(sentences_l2)\n",
    "    data_set = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
    "        pad_sentence_l1 = ([PAD_ID] * padding_l1) + sentences_l1[i]\n",
    "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
    "        pad_sentence_l2 = [GO_ID] + sentences_l2[i] + [EOS_ID] + ([PAD_ID] * padding_l2)\n",
    "\n",
    "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739e908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prepared minibatch with paddings and extra stuff\n",
      "En: [168, 1308, 1239, 5, 556, 955, 4]\n",
      "Fr: [1, 21, 472, 20, 5, 1378, 11, 489, 1643, 4, 2]\n",
      "# The sentence pass from X to Y tokens\n",
      "English: 7 -> 7\n",
      "French: 9 -> 11\n"
     ]
    }
   ],
   "source": [
    "data_set = prepare_sentences(\n",
    "    idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    ")\n",
    "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
    "print(\"En:\", data_set[0][0])\n",
    "print(\"Fr:\", data_set[0][1])\n",
    "print(\"# The sentence pass from X to Y tokens\")\n",
    "print(\"English:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
    "print(\"French:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585590d6",
   "metadata": {},
   "source": [
    "## training the Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b66536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:17:22.021173: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-21 12:17:22.029296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-21 12:17:22.037703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-21 12:17:22.040136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-21 12:17:22.048283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-21 12:17:22.583072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from seq2seq_model import Seq2SeqModel\n",
    "\n",
    "# from corpora_tools import *\n",
    "\n",
    "path_l1_dict = \"/tmp/l1_dict.p\"\n",
    "path_l2_dict = \"/tmp/l2_dict.p\"\n",
    "model_dir = \"/tmp/translate \"\n",
    "model_checkpoints = model_dir + \"/translate.ckpt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6710f8",
   "metadata": {},
   "source": [
    "This function returns the cleaned sentences, the dataset, the maximum length\n",
    "of the sentences, and the lengths of the dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f3154ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(use_stored_dictionary=False):\n",
    "    sen_l1, senl2 = retrieve_corpora()\n",
    "    clean_sen_l1 = [clean_sentences(s) for s in sen_l1]\n",
    "    clean_sent_l2 = [clean_sentences(s) for s in sen_l2]\n",
    "    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(\n",
    "        clean_sen_l1, clean_sen_l2\n",
    "    )\n",
    "\n",
    "    if not use_stored_dictionary:\n",
    "        dict_l1 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=15000, storage_path=path_l1_dict\n",
    "        )\n",
    "        dict_l2 = create_indexed_dictionary(\n",
    "            filt_clean_sen_l1, dict_size=10000, storage_path=path_l2_dict\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
    "        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
    "\n",
    "    dict_l1_length = len(dict_l1)\n",
    "    dict_l2_length = len(dict_l2)\n",
    "\n",
    "    data_set = prepare_sentences(\n",
    "        idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2\n",
    "    )\n",
    "    return (\n",
    "        (filt_clean_sen_l1, filt_clean_sen_l2),\n",
    "        data_set,\n",
    "        (max_length_l1, max_length_l2),\n",
    "        (dict_l1_length, dict_l2_length),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32fad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_checkpoints(model_dir, model_checkpoints):\n",
    "    for f in glob.glob(model_checkpoints + \"*\"):\n",
    "        os.remove(f)\n",
    "    try:\n",
    "        os.mkdir(model_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2f4e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b721e859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9c04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
